{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Embedding, SeparableConv1D, MaxPooling1D, GlobalAveragePooling1D, LSTM, Bidirectional\n",
    "from gensim.models import KeyedVectors, word2vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data_loader import load_imdb_sentiment_analysis_dataset\n",
    "\n",
    "file_path = '/Users/sakshamjain/Projects/AI/'\n",
    "\n",
    "(train_texts, train_labels), (test_texts, test_labels) = load_imdb_sentiment_analysis_dataset(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "        self.corpus = ' '.join(self.texts)\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def transform_to_lowercase(self, text=None):\n",
    "        return text.lower()\n",
    "    \n",
    "    def strip_html_tags(self, text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        stripped_text = soup.get_text()\n",
    "        return stripped_text\n",
    "        \n",
    "    def remove_accented_chars(self, text):\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return text\n",
    "    \n",
    "    def remove_special_characters(self, text):\n",
    "        text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
    "        return text\n",
    "        \n",
    "#     def contaction(self, text):\n",
    "#         return ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text(\" \")])\n",
    "\n",
    "    def clean(self, text=None, lower=True, strip_html=True, contract=True, remove_accented_chars=True,\n",
    "              special_char_removal=True, remove_stop_words=True):\n",
    "        if not text:\n",
    "            text = self.corpus\n",
    "        if lower:\n",
    "            text = self.transform_to_lowercase(text)\n",
    "        if strip_html:\n",
    "            text = self.strip_html_tags(text)\n",
    "        if remove_accented_chars:\n",
    "            text = self.remove_accented_chars(text)\n",
    "        if special_char_removal:\n",
    "            text = self.remove_special_characters(text)\n",
    "        if remove_stop_words:\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            cleaned_tokens = [word for word in tokens if word not in self.stop_words]\n",
    "            text = ' '.join(cleaned_tokens)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DataExplorer():\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "        self.corpus = ' '.join(self.texts)\n",
    "        \n",
    "    def get_num_words_per_sample(self):\n",
    "        numWords = []\n",
    "        for text in self.texts:\n",
    "            counter = len(text.split())\n",
    "            numWords.append(counter)  \n",
    "            \n",
    "        return numWords\n",
    "        \n",
    "    def get_median_num_words(self):\n",
    "        \"\"\"Returns the median number of words per sample given corpus.\n",
    "\n",
    "        # Arguments\n",
    "            sample_texts: list, sample texts.\n",
    "\n",
    "        # Returns\n",
    "            int, median number of words per sample.\n",
    "        \"\"\"\n",
    "        num_words = [len(s.split()) for s in self.texts]\n",
    "        return np.median(num_words)\n",
    "    \n",
    "    def plot_sample_length_distribution(self):\n",
    "        \"\"\"Plots the sample length distribution.\n",
    "\n",
    "        # Arguments\n",
    "            samples_texts: list, sample texts.\n",
    "        \"\"\"\n",
    "        plt.hist([len(s) for s in self.texts], 50)\n",
    "        plt.xlabel('Length of a sample')\n",
    "        plt.ylabel('Number of samples')\n",
    "        plt.title('Sample length distribution')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_frequency_distribution_of_ngram(self):\n",
    "        return None\n",
    "    \n",
    "    def plot_most_frequent_words(self):\n",
    "        # Visualization of the most frequent words\n",
    "        words = nltk.word_tokenize(self.corpus)\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        print('Number of tokens:', len(words))\n",
    "        print(\"List of 100 most frequent words/counts\")\n",
    "        print(fdist.most_common(100))\n",
    "        fdist.plot(40)\n",
    "        \n",
    "    def plot_most_frequent_words_preprocessed(self):\n",
    "        P = Preprocessor(self.texts)\n",
    "        prep_corpus = P.clean()\n",
    "        words = nltk.word_tokenize(prep_corpus)\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        print('Number of tokens:', len(words))\n",
    "        print(\"List of 100 most frequent words/counts\")\n",
    "        print(fdist.most_common(100))\n",
    "        fdist.plot(40)\n",
    "        \n",
    "    def get_corpus_statistics(self):\n",
    "        # Retrieve some info on the text data\n",
    "        num_texts = len(self.texts)\n",
    "        total_words = len(nltk.word_tokenize(self.corpus))\n",
    "        avg_words_text = self.get_median_num_words()\n",
    "        \n",
    "        print('Number of texts:', num_texts)\n",
    "        print('The total number of words in all texts', total_words)\n",
    "        print('The average number of words in each text is', avg_words_text)\n",
    "              \n",
    "        return num_texts, total_words, avg_words_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "    def __init__(self, train_texts, test_texts, max_features=20000, max_sequence_length=500):\n",
    "        self.train_texts = train_texts\n",
    "        self.test_texts = test_texts\n",
    "        self.preprocessed_train_corpus = Preprocessor(self.train_texts).clean()\n",
    "        self.preprocessed_test_corpus = Preprocessor(self.test_texts).clean()\n",
    "        self.max_features = max_features\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embed_dim = 300\n",
    "        \n",
    "    def get_custom_params(self, ngram_range=None, stop_words=None, min_df=None, max_dif=None, tokenizer=None,\n",
    "                          analyzer=None, preprocessor=None, lowercase=None, max_features=None, dtype=None, strip_accents=None):\n",
    "        params = {}\n",
    "        if ngram_range:\n",
    "            params['ngram_range'] = ngram_range\n",
    "        if stop_words:\n",
    "            params['stop_words'] = stop_words\n",
    "        if min_df:\n",
    "            params['min_df'] = min_df\n",
    "        if max_df:\n",
    "            params['max_df'] = max_df\n",
    "        if tokenizer:\n",
    "            params['tokenizer'] = tokenizer\n",
    "        if analyzer:\n",
    "            params['analyzer'] = analyzer\n",
    "        if preprocessor:\n",
    "            params['preprocessor'] = preprocessor\n",
    "        if lowercase:\n",
    "            params['lowercase'] = lowercase\n",
    "        if max_features:\n",
    "            params['lowercase'] = max_features\n",
    "        if dtype:\n",
    "            params['dtype'] = dtype\n",
    "        if strip_accents:\n",
    "            params['strip_accents'] = strip_accents\n",
    "            \n",
    "        return params\n",
    "    \n",
    "    def get_vector_info(vector):\n",
    "        matrix = vector.toarray()\n",
    "        shape = vector.shape\n",
    "        return matrix, shape\n",
    "    \n",
    "    def count_vectorize(self, kwargs):\n",
    "        vectorizer = CountVectorizer(**kwargs)\n",
    "        train_vector = vectorizer.fit_transform(self.train_texts)\n",
    "        test_vector = vectorizer.transform(self.test_texts)\n",
    "        # List of features (Words)\n",
    "        features = vectorizer.get_feature_names()\n",
    "        # Index assigned for every token\n",
    "        vocabulary = vectorizer.vocabulary_\n",
    "        return train_vector, test_vector, vocabulary\n",
    "        \n",
    "    def tfidf_vectorize(self, kwargs):\n",
    "        vectorizer = TfidfVectorizer(**kwargs)\n",
    "        train_vector = vectorizer.fit_transform(self.train_texts)\n",
    "        test_vector = vectorizer.transform(self.test_texts)\n",
    "        # List of features (Words)\n",
    "        features = vectorizer.get_feature_names()\n",
    "        # Index assigned for every token\n",
    "        vocabulary = vectorizer.vocabulary_\n",
    "        return train_vector, test_vector, vocabulary\n",
    "    \n",
    "    def sequence_vectorize(self):\n",
    "        # Create vocabulary with training texts.\n",
    "        tokenizer = Tokenizer(num_words=self.max_features)\n",
    "        tokenizer.fit_on_texts(pd.Series(self.preprocessed_train_corpus))\n",
    "        \n",
    "        # Vectorize texts\n",
    "        train_vector = tokenizer.texts_to_sequences(pd.Series(self.preprocessed_train_corpus))\n",
    "        test_vector = tokenizer.texts_to_sequences(pd.Series(self.preprocessed_test_corpus))\n",
    "        \n",
    "        # Get max sequence length.\n",
    "        max_length = len(max(train_vector, key=len))\n",
    "        if max_length > self.max_sequence_length:\n",
    "            max_length = self.max_sequence_length\n",
    "        \n",
    "        # Fix sequence length to max value. Sequences shorter than the length are\n",
    "        # padded in the beginning and sequences longer are truncated at the beginning.\n",
    "        train_vector = pad_sequences(train_vector, maxlen=max_length)\n",
    "        test_vector = pad_sequences(test_vector, maxlen=max_length)\n",
    "        # Index assigned for every token\n",
    "        vocabulary = tokenizer.word_index\n",
    "        \n",
    "        return train_vector, test_vector, vocabulary\n",
    "    \n",
    "    def word_embedding_vectorize(self, vocab):\n",
    "        word2vec = KeyedVectors.load_word2vec_format('Data/GoogleNews-vectors.bin.gz',binary=True)\n",
    "        \n",
    "        # Construct the embedding weights matrix\n",
    "        # Where rows is length of vocab + 1\n",
    "        # And column is value of embed_dim\n",
    "        embedding_weights = np.zeros((len(vocab) + 1, self.embed_dim))\n",
    "        # Creating a dictionary item of vocab\n",
    "        for word, index in vocab.items():\n",
    "            embedding_weights[index, :] = word2vec[word] if word in word2vec else np.random.rand(self.embed_dim)\n",
    "        \n",
    "        # Constructing word-vector dictionary\n",
    "        word_vector_dict = dict(zip(pd.Series(list(vocab.keys())),\n",
    "                                    pd.Series(list(vocab.keys())).apply(\n",
    "                                        lambda x: features_embedding_weights[vocab[x]]\n",
    "                                    )))\n",
    "        \n",
    "        return embedding_weights, word_vector_dict\n",
    "    \n",
    "    def tfidf_embedding_vectorize(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, train_texts, train_labels, test_texts, test_labels, word_embedding=False):\n",
    "        self.train_texts = train_texts\n",
    "        self.train_labels = train_labels\n",
    "        self.test_texts = test_texts\n",
    "        self.test_labels = test_labels\n",
    "        self.vocab = None\n",
    "        self.vectorizer = None\n",
    "        self.train_vector = None\n",
    "        self.test_vector = None\n",
    "        self.prediction = None\n",
    "        self.max_features = 20000\n",
    "        self.ngram_range = (1,2)\n",
    "        self.word_embedding = word_embedding\n",
    "        \n",
    "    def vectorize(self):\n",
    "        V = Vectorizer(train_texts=self.train_texts, test_texts=self.test_texts, max_features=self.max_features)\n",
    "        if not self.word_embedding:\n",
    "            self.train_vector, self.test_vector, self.vocab = V.tfidf_vectorize(\n",
    "                {'strip_accents': 'unicode',\n",
    "                 'analyzer': 'word',\n",
    "                 'ngram_range': self.ngram_range,\n",
    "                 'min_df': 2,\n",
    "                 'max_features': self.max_features\n",
    "                })\n",
    "        else:\n",
    "            # code here\n",
    "            return\n",
    "        \n",
    "    def run(self, classifier):\n",
    "        self.vectorize()\n",
    "        model = classifier().fit(self.train_vector, self.train_labels)\n",
    "        self.prediction = model.predict(self.test_vector)\n",
    "        print(confusion_matrix(self.test_labels, self.prediction))  \n",
    "        print(classification_report(self.test_labels, self.prediction))  \n",
    "        print(accuracy_score(self.test_labels, self.prediction))\n",
    "        \n",
    "\n",
    "class NNModel():\n",
    "    def __init__(self, num_classes, train_texts, train_labels, test_texts, test_labels):\n",
    "        self.num_classes = num_classes\n",
    "        self.train_texts = train_texts\n",
    "        self.train_labels = train_labels\n",
    "        self.train_vector = None\n",
    "        self.test_texts = test_texts\n",
    "        self.test_labels = test_labels\n",
    "        self.test_vector = None\n",
    "        self.vocab = None\n",
    "        #\n",
    "        self.max_features = 20000\n",
    "        self.max_sequence_length = 500\n",
    "        #\n",
    "        DE = DataExplorer(self.train_texts)\n",
    "        self.num_texts, self.total_words, self.avg_words_text = DE.get_corpus_statistics()\n",
    "        self.S_by_W = self.num_texts / self.avg_words_text\n",
    "        # Layer's Params\n",
    "        self.input_shape = None\n",
    "        self.filters = None\n",
    "        self.kernal_size = 3\n",
    "        self.pool_size = None\n",
    "        self.dropout_rate = 0.2\n",
    "        self.units = 64\n",
    "        self.last_layer_units = None\n",
    "        self.last_layer_activation = None\n",
    "        # Embedding\n",
    "        self.embed_dim = 300\n",
    "        self.embedding_weights = None\n",
    "        self.word_vect_dic = None\n",
    "        #\n",
    "        self.optimizer = 'adam'\n",
    "        self.metric = 'accuracy'\n",
    "        self.loss = None\n",
    "        self.learning_rate = 1e-3\n",
    "        self.epochs = 500\n",
    "        self.batch_size = 128\n",
    "        \n",
    "    \n",
    "    def set_params(self, input_shape=None, filters=None, units=None,\n",
    "                   kernal_size=None, pool_size=None, dropout_rate=None,\n",
    "                   learning_rate=None, epochs=None, batch_size=None, embed_dim=None):        \n",
    "        if input_shape:\n",
    "            self.input_shape = input_shape\n",
    "        if filters:\n",
    "            self.filters = filters\n",
    "        if units:\n",
    "            self.units = units\n",
    "        if kernal_size:\n",
    "            self.kernal_size = kernal_size\n",
    "        if pool_size:\n",
    "            self.pool_size = pool_size\n",
    "        if dropout_rate:\n",
    "            self.dropout_rate = dropout_rate\n",
    "        if learning_rate:\n",
    "            self.learning_rate = learning_rate\n",
    "        if epochs:\n",
    "            self.epochs = epochs\n",
    "        if batch_size:\n",
    "            self.batch_size = batch_size\n",
    "        if embed_dim:\n",
    "            self.embed_dim = embed_dim\n",
    "            \n",
    "        if self.num_classes == 2:\n",
    "            self.last_layer_activation = 'sigmoid'\n",
    "            self.last_layer_units = 1\n",
    "            self.loss = 'binary_crossentropy'\n",
    "        elif self.num_classes > 2:\n",
    "            self.last_layer_activation = 'softmax'\n",
    "            self.last_layer_units = self.num_classes\n",
    "            self.loss = 'sparse_categorical_crossentropy'\n",
    "        else:\n",
    "            print('ERROR')\n",
    "            \n",
    "    def vectorize(self):\n",
    "        V = Vectorizer(train_texts=self.train_texts, test_texts=self.test_texts, max_features=self.max_features)\n",
    "        if self.S_by_W < 1500:\n",
    "            self.train_vector, self.test_vector, self.vocab = V.tfidf_vectorize(\n",
    "                {'strip_accents': 'unicode',\n",
    "                 'analyzer': 'word',\n",
    "                 'ngram_range': (1, 2),\n",
    "                 'min_df': 2,\n",
    "                 'max_features': self.max_features\n",
    "                })\n",
    "        else:\n",
    "            self.train_vector, self.test_vector, self.vocab = V.sequence_vectorize()\n",
    "            self.embedding_matrix, self.word_vect_dic = V.word_embedding_vectorize(self.vocab)\n",
    "        \n",
    "        \n",
    "    def get_Embedding(self, use_pretrained_embedding=False, is_embedding_trainable=False):\n",
    "        if use_pretrained_embedding:\n",
    "            layer = Embedding(\n",
    "                input_dim=len(self.vocab) + 1,\n",
    "                output_dim=self.embed_dim,\n",
    "                input_length=self.max_sequence_length,\n",
    "                weights=[self.embedding_matrix],\n",
    "                trainable=is_embedding_trainable\n",
    "               )\n",
    "        else:\n",
    "            layer = Embedding(\n",
    "                input_dim=num_features,\n",
    "                output_dim=self.embed_dim,\n",
    "                input_length=self.input_shape[0]\n",
    "            )\n",
    "         \n",
    "        return layer\n",
    "    \n",
    "    def get_SeparableConv1D(n):\n",
    "        layer = SeparableConv1D(\n",
    "            filters=self.filters * n,\n",
    "            kernel_size=self.kernel_size,\n",
    "            activation='relu',\n",
    "            bias_initializer='random_uniform',\n",
    "            depthwise_initializer='random_uniform',\n",
    "            padding='same'\n",
    "        )\n",
    "        return layer\n",
    "    \n",
    "    def get_Conv1D(n):\n",
    "        layer = Conv1D(\n",
    "            filters=self.filters * n,\n",
    "            kernel_size=self.kernel_size,\n",
    "            activation='relu',\n",
    "        )\n",
    "        return layer\n",
    "    \n",
    "    def build_cnn_model(self, layers, n):\n",
    "        \"\"\"\n",
    "        Convolutional Neural Network\n",
    "        \"\"\"\n",
    "        model = models.Sequential()\n",
    "        model.add(self.get_Embedding())\n",
    "        \n",
    "        for _ in range(layers - 1):\n",
    "            model.add(self.get_Conv1D(n))\n",
    "            model.add(MaxPooling1D(pool_size=self.pool_size))\n",
    "            \n",
    "        model.add(Dense(units=self.units, ativation='relu'))\n",
    "        model.add(Dense(units=self.last_layer_units, activation=self.last_layer_activation))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_bidirectional_lstm(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        model = models.Sequential()\n",
    "        model.add(self.get_Embedding())\n",
    "        model.add(Bideractional(LSTM(self.units)))\n",
    "        model.add(Dropout(rate=self.dropout_rate))\n",
    "        model.add(Dense(units=self.last_layer_units, activation=self.last_layer_activation))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_mlp_model(self, layers):\n",
    "        \"\"\"\n",
    "        Multi Layer Perceptrons (MLPs)\n",
    "        \"\"\"\n",
    "        model = models.Sequential()\n",
    "        model.add(Dropout(rate=self.dropout_rate, input_shape=self.input_shape))\n",
    "        \n",
    "        for _ in range(layers-1):\n",
    "            model.add(Dense(units=self.units, activation='relu'))\n",
    "            model.add(Dropout(rate=self.dropout_rate))\n",
    "        \n",
    "        model.add(Dense(units=self.last_layer_units, activation=self.last_layer_activation))\n",
    "        return model\n",
    "    \n",
    "    def build_sepcnn_model(self, blocks):\n",
    "        \"\"\"\n",
    "        Separable Convolutional Network\n",
    "        \"\"\"\n",
    "        model = models.Sequential()\n",
    "        model.add(self.get_Embedding())\n",
    "        \n",
    "        for _ in range(blocks - 1):\n",
    "            model.add(Dropout(rate=self.dropout_rate))\n",
    "            model.add(self.get_SeparableConv1D(1))\n",
    "            model.add(self.get_SeparableConv1D(1))\n",
    "            model.add(MaxPooling1D(pool_size=pool_size))\n",
    "            \n",
    "        model.add(self.get_SeparableConv1D(2))\n",
    "        model.add(self.get_SeparableConv1D(2))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        model.add(Dense(units=self.last_layer_units, activation=self.last_layer_activation))\n",
    "            \n",
    "        return model\n",
    "    \n",
    "    def run(self):\n",
    "        self.set_params()\n",
    "        self.vectorize()\n",
    "        \n",
    "        if self.S_by_W < 1500:\n",
    "            \"\"\"\n",
    "            N-gram Model\n",
    "            \"\"\"\n",
    "            self.input_shape = self.train_vector.shape[1:]\n",
    "            model = self.build_mlp_model(layers=2)\n",
    "            model.compile(optimizer=self.optimizer, loss=self.loss, metrics=[self.metric])\n",
    "            print(model)\n",
    "        \n",
    "            history = model.fit(\n",
    "            self.train_vector,\n",
    "            self.train_labels,\n",
    "            epochs=self.epochs,\n",
    "            validation_data=(self.test_vector, self.test_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=self.batch_size).history\n",
    "        \n",
    "            print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "                acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "        else:\n",
    "            \"\"\"\n",
    "            Sequence Model\n",
    "            \"\"\"\n",
    "            self.build_sepcnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORING DATA\n",
    "de_TRAIN = DataExplorer(train_texts)\n",
    "\n",
    "de_TRAIN.get_corpus_statistics()\n",
    "\n",
    "de_TRAIN.plot_most_frequent_words()\n",
    "de_TRAIN.plot_most_frequent_words_preprocessed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "V = Vectorizer(train_texts=train_texts, test_texts=test_texts)\n",
    "\n",
    "# TFIDF\n",
    "# train_vector_tfidf, test_vector_tfidf, vocab = V.tfidf_vectorize({})\n",
    "# print(train_vector_tfidf)\n",
    "# print(train_vector_tfidf.shape)\n",
    "\n",
    "# SEQUENCE\n",
    "train_vector, test_vector, vocab = V.sequence_vectorize()\n",
    "word2vec = KeyedVectors.load_word2vec_format('Data/GoogleNews-vectors.bin.gz',binary=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
